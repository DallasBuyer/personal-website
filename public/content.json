{"meta":{"title":"DallasBuyer","subtitle":"artificial intelligence","description":null,"author":"PengXu","url":"http://yoursite.com","root":"/"},"pages":[{"title":"Biography","date":"2019-04-24T04:54:31.000Z","updated":"2019-05-29T11:50:30.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"Peng Xu 许鹏Postgraduate StudentSoftware EngineeringSchool of Digital Media, Jiangnan UniversityEmail: 6171610015 [AT] stu.jiangnan.edu.cnWeChat: xu172447385 InterestsI am interested in artificial intelligence, transfer learning, multi-view learning, fuzzy machine learning and interpretable machine learning. TalksSpotlight Talk on the Thirty-Third AAAI Conference on Artificial Intelligence, Hawaii, USA, 01/2019 ReviewerJournalsIEEE Transactions on Fuzzy SystemIEEE Transactions on Emergin Topics in Computational IntelligenceIEEE Transactions on Systems, Man and Cybernetics: SystemsNeurocomputing ConferenceThe Third International Conference on Biological Information and Biomedical Engineering (BIBE 2019)"},{"title":"categories","date":"2019-04-25T10:41:10.000Z","updated":"2019-04-25T11:22:26.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Publications and Code","date":"2019-04-25T10:41:10.000Z","updated":"2019-06-01T12:01:26.000Z","comments":true,"path":"publications/index.html","permalink":"http://yoursite.com/publications/index.html","excerpt":"","text":"Concise Fuzzy System Modeling Integrating Soft Subspace Clustering and Sparse LearningPeng Xu, Zhaohong Deng, Chen Cui, Te Zhang, Kup-Sze Choi, Suhang Gu, Shitong WangIEEE Transactions on Fuzzy System, 2019. [PDF] Transfer Representation Learning with TSK Fuzzy SystemPeng Xu, Zhaohong Deng, Jun Wang, Qun Zhang,Shitong WangarXiv:1901.02703, 2019. [PDF] Multi-view Information-theoretic Co-clustering for Co-occurrence DataPeng Xu, Zhaohong Deng, Kup-Sze Choi, Longbin Cao, Shitong WangAAAI (spotlight), 2019. [PDF] [CODE] Joint Information Preservation for Heterogeneous Domain AdaptationPeng Xu, Zhaohon Deng, Kup-Sze Choi, Jun Wang, Shitong WangTechical Report, 2019. [PDF] Deep Image Feature Learning with Fuzzy FulesXiang Ma, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangTechical Report, 2019. [PDF] Transductive Joint-Knowledge-Transfer TSK FS for Recognition of Epileptic EEG SignalsZhaohong Deng, Peng Xu, Lixiao Xie, Kup-Sze Choi, Shitong WangIEEE Transactions on Neural Systems and Rehabilitation Engineering, 2018. [PDF] Genralized Hidden-Mapping Transductive Transfer Learning for Recognition of Epileptic Electroencephalogram SignalsLixiao Xie, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangIEEE Transactions on Cybernetics, 2018. [PDF]"}],"posts":[{"title":"Search Algorithms based on Linear List","slug":"find","date":"2019-06-16T07:58:34.000Z","updated":"2019-07-12T04:10:04.721Z","comments":true,"path":"2019/06/16/find/","link":"","permalink":"http://yoursite.com/2019/06/16/find/","excerpt":"顺序查找顺序查找一般是针对无序表的线性查找，遍历所有元素。最好的情况是第一个位置就找到了，这时候算法的时间复杂度为 $O(1)$，最坏的情况是遍历到最后一个位置才找到，这时候时间复杂度为 $O(n)$，所以平均查找次数为 $(n+1)/2$。最终时间复杂度为 $O(n)​$。 1234567891011121314151617class Find: def seqSearch(self, lists, key): length = len(lists) if length == 0: return False for i in range(length): if lists[i] == key: return i else: return Falseif __name__ == \"__main__\": lists = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] find = Find() results = find.seqSearch(lists, 123) print(results)","text":"顺序查找顺序查找一般是针对无序表的线性查找，遍历所有元素。最好的情况是第一个位置就找到了，这时候算法的时间复杂度为 $O(1)$，最坏的情况是遍历到最后一个位置才找到，这时候时间复杂度为 $O(n)$，所以平均查找次数为 $(n+1)/2$。最终时间复杂度为 $O(n)​$。 1234567891011121314151617class Find: def seqSearch(self, lists, key): length = len(lists) if length == 0: return False for i in range(length): if lists[i] == key: return i else: return Falseif __name__ == \"__main__\": lists = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] find = Find() results = find.seqSearch(lists, 123) print(results) 二分查找二分查找又叫折半查找，优点是比较次数少，查找速度快，平均性能好。缺点是要求待查表为有序表，且插入删除操作困难。所以折半查找适用于不经常变动而查找频繁的有序列表。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。最好的情况时间复杂度为 $O(1)$，最坏的情况时间复杂度为 $O(logn)​$。下面是二分查找的两种Python实现，一种递归一种非递归。 12345678910111213141516171819202122232425262728293031323334class Find: def binarySearch(self, lists, key): length = len(lists) if length == 0: return False begin = 0 end = length - 1 while begin &lt;= end: mid = (begin + end) // 2 if lists[mid] &gt; key: end = mid - 1 elif lists[mid] &lt; key: begin = mid + 1 else: return True return False def binaryRecursiveSearch(self, lists, key): length = len(lists) if length == 0: return False mid = length // 2 if lists[mid] &gt; key: return self.binaryRecursiveSearch(lists[0:mid], key) elif lists[mid] &lt; key: return self.binaryRecursiveSearch(lists[mid+1:], key) else: return Trueif __name__ == \"__main__\": lists = [1, 5, 8, 123, 22, 54, 7, 99, 300, 222] find = Find() results = find.binaryRecursiveSearch(lists, 54) print(results) 插入查找斐波那契查找散列表查找(哈希表)","categories":[{"name":"data structures and algorithms","slug":"data-structures-and-algorithms","permalink":"http://yoursite.com/categories/data-structures-and-algorithms/"}],"tags":[{"name":"data structure, algorithm","slug":"data-structure-algorithm","permalink":"http://yoursite.com/tags/data-structure-algorithm/"}]},{"title":"Exploring Randomly Wired Neural Networks","slug":"random network generator","date":"2019-05-03T12:45:32.000Z","updated":"2019-05-29T11:44:08.000Z","comments":true,"path":"2019/05/03/random network generator/","link":"","permalink":"http://yoursite.com/2019/05/03/random network generator/","excerpt":"Summary不同与以往的网络结构搜索算法，本文没有使用强化学习的方式令搜索出来的网络结构分类精度的reward很高。而是加不受限制的随机连接结构有怎样的表现。实验结果显示随机网络生成器所生成的网络结构的表现大多情况下能和人为设计的，通过NAS搜索得到的结果相媲美。一方面这篇文章提出了这种新的思想，就是设计不同的网络生成器；另一方面，结果表明用RL方式的暴力搜索其实并没有很大的优势，距离AutoML也还很遥远。由此启发我们从设计不同的搜索算法，搜索空间转而研究设计具有不同的先验知识的网络生成器。","text":"Summary不同与以往的网络结构搜索算法，本文没有使用强化学习的方式令搜索出来的网络结构分类精度的reward很高。而是加不受限制的随机连接结构有怎样的表现。实验结果显示随机网络生成器所生成的网络结构的表现大多情况下能和人为设计的，通过NAS搜索得到的结果相媲美。一方面这篇文章提出了这种新的思想，就是设计不同的网络生成器；另一方面，结果表明用RL方式的暴力搜索其实并没有很大的优势，距离AutoML也还很遥远。由此启发我们从设计不同的搜索算法，搜索空间转而研究设计具有不同的先验知识的网络生成器。 Motivation现在大多数神经网络的成功在于其复杂的网络结构设计，为了探索网络结构对于深度学习的优势，网络结构搜索成了近一两年较为热门的方向。网络结构搜索 (neural architecture search, NAS)致力于同时优化网络的连接机构和操作算子，然而这类算法中网络结构的搜索过程都是认为设计的，所以网络结构的空间受到了很大的限制。所以，本文通过随机连接网络探索了更加多样性的网络结构。整个模型可以叫做随机网络生成器，实验结果显示，通过探索在更加不受约束的搜索空间中设计网络生成器会有不错的前景。 试验结果显示： 这些随机网络生成器的几个不同变种所产生的网络结构，都可以在Imagenet上面展现出竞争性的优势。其中最好的生成器能产生一些可以媲美甚至超过完全人为设计的或者是通过NAS搜索得到的网络结构。 同一个网络生成器所生成的网络精度方差较低。 不同的网络生成器生成的网络之间精度有明显差别。 上述讨论表明网络生成器的设计很关键。 Methodology为了扩大网络生成器的搜索空间，本文提出了随机连接神经网络。 先简要定义一下网络生成器，给定参数 $\\theta$ ，网络生成器 $g$ 会生成一个确定的网络，每次给定相同的 $\\theta$，$g$ 一定能产生相同的网络结构。如果网络生成器可以接受另外一个随机种子参数 $s$，通过这个随机种子，多次运行 $g(\\theta,s)$ 网络生成器可以产生一族网络结构，我们就叫这个生成器为随机网络生成器 $g(\\theta, s)$。(这里有个问题，每次给定相同的随机种子，随机网络生成器是不是也可以产生不同的网络结构)文献[1]中使用了循环神经网络RNN作为网络生成器，这里RNN可以输出一些列字符串，这些字符串定义的就是网络结构的超参数(比如，卷积网络每一层的kernel width，kernel height，stride width stride height，filter number)。这里训练这个神经网络是个难点，因为这里并不像传统RNN作为seq2seq模型有输入序列的监督信息可以使用反向传播来训练。本文作者采用了强化学习的策略梯度算法(policy gradient algorithm)中的REIFORCE算法，即使用reward来训练模型。本文中的reward指的是用这个RNN生成一个字符串网络后，训练并测试这个网络，这个网络的精度就是reward，而这个网络结构其实就是强化学习模型中的策略，也就是我们最后想学习的内容。(这里怎么用上述所提算法更新整个模型是个难点)文献[2]中也是使用循环神经网络作为网络生成器，它和[1]不同的是，它生成的是一个cell，这个cell由5个参数构成，即RNN可以生成5个参数，对应于5种不同的操作，这个RNN生成的每种操作的输出都是一个softmax分类层，类别是我们预先定义好的不同的操作类型。所以本文做的网络搜索其实是在搜索一种有效的cell结构，之后再将很多个cell拼接起来构成一个网络。这里采用的学习算法和[1]中一样，都是REINFORCE算法。上述两种网络搜索框架的缺陷是，它们都通过预定义的内容限制了网络的搜索空间，比如文献[1]中的搜索方式虽然可以产生多变的卷积核大小，步长，通道数，但是层与层之间的连接方式很受限制。文献[2]中的搜索结构是cell，预定义的内容直接限制了cell的构成结构，只不过一个cell中的操作算法可以变化。总之传统的NAS算法的搜索空间会被很多先验知识所限定。本文提出了随机连接网络，从而使得网络结构的搜索空间受到更少的限制，但是这里其实操作算子还是受到一定程度的限制。 Random Neural Generator本文使用随机图定义的随机网络生成器来生成不同的网络结构，本文选择了三种不同的已有随机图模型，在生成图模型之后，将图模型转化成网络结构。所以本文的两个核心问题点就是： 使用什么样的随机图模型生成随机图结构 怎么将图结构转化成对应的网络结构 Random Graph本文选择了三种随机图模型，分别是ER，BA，WS，下面简要讲解以下三种随机图模型。 ER是最简单的随机图模型，它假设给定N个节点，然后迭代地以概率P对任意两个节点进行连接，在遍历完所有的配对节点之后救生成了一个随机图。所以这个随机模型只有一个参数就是P。 BA算法以添加新节点的方式生成随机图模型，假设一开始有M个节点，它们之间没有任何边的连接。接下来每新添加一个节点就添加M条新边，怎么添加这M条边那，当将新添加的节点连接到点 $v$ 时，它连接到这个点的概率正比于这个点的degree。就这样重复这个过程知道这个点拥有M条边，之后以同样的方式添加另外的节点，直到一共有N个节点。所以最终生成的图一共有$M*(N-M)$ 条边。所以这个随机模型的唯一参数就是M。 WS是它们三个中较为复杂的一个随机模型。首先N个节点呈环形摆放，给定一个偶数K，每个节点都与其相邻的K/2个节点相连接。之后按顺时针顺序，对于每个节点 $v$，连接这个节点和它下面的第 $i$ 个节点的边都以概率 $P$ 重新连接。这里第 $i$ 个节点是以均匀分布的方式随机选择的，不过即不选择节点 $v$ 也不选择重复的边，这个选择过程重复K/2次。所以这个随机模型的参数有两个，分别是K和P。 Generate Network Structures from Graphs用上述三种方式生成图之后是无向图，首先我们需要将其转化成有向图模型。转换时现将每个图的所有节点附上序号，然后根据这些序号，使得每条边的方向都是从序号小的节点指向序号大的节点。那又按照什么策略给这些点赋序号那：对于ER模型，随机赋值。对于BA模型，初始的M个节点赋值为1到M，其余节点按其添加顺序赋值。WS模型，按照顺时针方向对所有节点赋值。得到有向图之后，就要将其转化成网络结构。这里假设边就是数据流，代表一个节点的数据流向两外一个节点。节点就是运算操作，每个节点都会受到多条边的输入，也会产生多条边的输出。这里我们假设一个节点就由三种操作构成，分别是聚合操作(aggregation)，转换操作(transformation)和分发操作(distribution)。聚合操作就是几条输入边的数据的加权和，这里存在可学习的权值。转换操作就是一个ReLU-Conv-BN的三联操作。分发操作就是将节点数据复制多份分别传输到不同的边上。由上述过程可知，其实虽然所提算法在网络结构上的搜索空间更大了，但是在操作上也严重限制了搜索空间，因为由生成的图转化成网络的过程中，所有运算都是固定的。 这里为了提高模型的能力，进行的分阶段构造，上述过程生成的一个图算是传统网络的一层，这样堆叠多个图，就构成了一个多层的网络。 Design and Optimization整个算法过程涉及的参数其实并不多。有每个图的节点数N，和每种随机图的参数。因为我们最终作了图的堆叠，所以还需要设置堆叠的图的个数，还有每个节点中的卷积操作的输出通道数和卷积步长。 Reference[1] Zoph B, Le Q V. Neural Architecture Search with Reinforcement Learning[J]. international conference on learning representations, 2017.[2] Zoph B, Vasudevan V, Shlens J, et al. Learning Transferable Architectures for Scalable Image Recognition[J]. computer vision and pattern recognition, 2018: 8697-8710.[3] Exploring Randomly Wired Neural Networks for Image Recognition. CoRR: abs/1904.01569","categories":[{"name":"neural architecture search","slug":"neural-architecture-search","permalink":"http://yoursite.com/categories/neural-architecture-search/"}],"tags":[{"name":"random neural network, neural architecture search","slug":"random-neural-network-neural-architecture-search","permalink":"http://yoursite.com/tags/random-neural-network-neural-architecture-search/"}]},{"title":"Co-Transfer Learning Using Coupled Markov Chains with Restart","slug":"Co-Transfer-Learning","date":"2019-04-25T08:21:47.000Z","updated":"2019-05-25T13:09:42.000Z","comments":true,"path":"2019/04/25/Co-Transfer-Learning/","link":"","permalink":"http://yoursite.com/2019/04/25/Co-Transfer-Learning/","excerpt":"Summary个人感觉这篇文章很有新意，扩展了传统的迁移学习场景，和归纳式迁移学习一样，假设目标域也有标签数据，但是它的创新点在于co-transfer，即进行了双向迁移，使得任何一个领域都可以辅助其他领域迁移。而作者实现co-transfer在于利用了两种关系，一种是Intra-relationship，这里可以直接在同一个特征空间内用距离度量来构造；另外一种是inter-relationships，这里很巧妙地借助了co-occurrence information，注意这里的co-occurrence和多视角学习中的instance-level co-occurrence并不相同。之后将两种关系合并构造一个coupled Markov chain形成了一个联合转移概率图，之后通过向标签分布的概率转移来实现标签预测，这里有趣的是还引入了多标签和标签ranking的概念，有兴趣的同学也可以借鉴这种思想解决多标签分类的问题。","text":"Summary个人感觉这篇文章很有新意，扩展了传统的迁移学习场景，和归纳式迁移学习一样，假设目标域也有标签数据，但是它的创新点在于co-transfer，即进行了双向迁移，使得任何一个领域都可以辅助其他领域迁移。而作者实现co-transfer在于利用了两种关系，一种是Intra-relationship，这里可以直接在同一个特征空间内用距离度量来构造；另外一种是inter-relationships，这里很巧妙地借助了co-occurrence information，注意这里的co-occurrence和多视角学习中的instance-level co-occurrence并不相同。之后将两种关系合并构造一个coupled Markov chain形成了一个联合转移概率图，之后通过向标签分布的概率转移来实现标签预测，这里有趣的是还引入了多标签和标签ranking的概念，有兴趣的同学也可以借鉴这种思想解决多标签分类的问题。 Scene and Motivation本文提出了一个叫作Co-Transfer Learning的算法，这里的Co-Transfer意为协同迁移。传统的迁移学习场景一般假设利用包含大量有标签数据的源域辅助只包含少量有标签数据的目标域数据进行建模，之后对目标域的无标签数据进行分类。显然传统的迁移学习是单向的，即源域辅助目标域。而本文的Co-Transfer的出发点是能不能作多向的迁移，即没有源域和目标域之分。假设同时有多个领域的数据，其中每个领域都包含一部分有标签数据和一部分无标签数据，本文的目标就是这些领域互相迁移。举个例子，假设现在有文本和图像两个领域的数据，每个领域的数据都包含有标签和无标签两种数据，本文想通过某种方式为这两个领域的数据建立某种桥梁，从而让图像领域的数据辅助文本领域数据的建模，同时文本领域的数据也可以辅助图像领域的建模。如果存在多个领域的话就是多向的，每个领域都可以辅助其它任何一个领域进行建模，从而形成了Co-Transfer的思想。 Problem Definition有了以上出发点，现在就要把问题数学化。首先定义多个领域，这里每个领域定义为一个样本空间和一个特征空间，且每个领域的样本空间和特征空间都不同。将第 $i$ 个样本空间表示为 $\\mathcal{X^{(i)}}$，第 $i$ 个特征空间表示为 $\\mathcal{Y^{(i)}}$，则第 $i$ 个样本空间的某个样本表示为 $x_k^{(i)}$，其对应的特征向量表示为 $\\mathbf{y}_k^{(i)}$ 。且对于任意的 $i\\neq i’$，有 $\\mathcal{X^{(i)}}\\neq \\mathcal{X^{(i’)}}$ 且 $\\mathcal{Y^{(i)}}\\neq \\mathcal{Y^{(i’)}} $，即每两个领域的样本空间和特征空间均不相同。每个领域都存在一个有标签样本集 $\\mathcal{L^{(i)}}$ 和一个无标签样本集 $\\mathcal{U^{(i)}}$。其中 $\\mathcal{L^{(i)}}$ 可用式(1)表示，${\\bar n_i}$ 第 $i$ 个领域有标签样本的个数。$\\mathcal{U^{(i)}}$ 可用式(2)表示，$\\hat n_i$ 表示第 $i$ 个领域无标签样本的个数。Co-Transfer Learning的目的就是使用所有领域中的有标签样本同时训练 $N$ 个不同的分类器来预测无标签测试样本的类别。$$\\mathcal {L^{(i)}}={ x_k^{(i)},C_k^{(i)} }_{k=1}^{\\bar n_i}$$ $$\\mathcal {U^{(i)}}={ x_k^{(i)} }_{k=\\bar n_i+1}^{\\bar n_i+\\hat n_i}$$ The Proposed Framework本文所提方法将问题建模成一个所有样本的联合转移概率图(joint transition probability graph)模型，这样就可以利用topic-sensitive PageRank和random walk with restart的思想来解决co-transfer的问题了。其中transition probabilities可以通过intra-relationships和inter-relationships来构造，这里intra-relationships是指在同一个样本空间内所有样本的关系，它可以基于样本之间的affinity metric来构造，inter-relationships是指在不同样本空间的样本之间的关系，它可以基于样本之间的co-occurrence information来构造。未完待续… Intrarelationships and Interrelationships式(3)展示出了所有 $N$ 个样本空间的所有样本(有标签和无标签)，第 $i$ 个样本空间的所有样本数为 $n_i=\\bar n_i+\\hat n_i$。为了简化表示，我们设 $n=\\sum _{i=1}^Kn_i$。$$\\underbrace{x_1^{(1)},\\dots,x_{n_1}^{(1)}}_{\\text{1st instance space}},\\underbrace{x_1^{(2)},\\dots,x_{n_2}^{(2)}}_{\\text{2st instance space}},\\dots,\\underbrace{x_1^{(N)},\\dots,x_{n_N}^{(N)}}_{\\text{Nst instance space}}$$ Intra-relationship首先我们定义在同一个样本空间内的所有样本之间的intra-relationships，在第 $i$ 个样本空间内的样本 $x_k^{(i)}$ 和 $x_l^{(i)}$ 之间的affinity metric定义为 $a_{k,l}^{(i,i)}$ ， $a_{k,l}^{(i,i)}$ 可以基于两个样本在特征空间的特征表示来计算，如式(4)。$$a_{k,l}^{(i,i)}=exp[\\frac {-\\Vert \\mathbf y_k^{(i)}-\\mathbf y_l^{(i)}\\Vert_2}{2\\sigma^2}]$$从而所有的 $a_{k,l}^{(i,i)}$ 可以组合形成矩阵 $\\mathbf A^{(i,i)}$，其大小为 $n_i-\\text{by}-n_i$，其中的每个元素都代表此样本空间内两个样本之间的intra-relationship。这里重点来了，我们根据矩阵 $\\mathbf A^{(i,i)}$ 就可以构造一个Markov transition probability matrix $\\mathbf P^{(i,i)}$，怎么构造那，可以对 $\\mathbf A^{(i,i)}$ 的每一列进行归一化从而得到 $\\mathbf P^{(i,i)}$，使得 $\\mathbf P^{(i,i)}$ 的每一列元素的和都为1。因为这里矩阵 $\\mathbf A^{(i,i)}$ 是对称的，所以得到的 $\\mathbf P^{(i,i)}$ 不仅列实现了归一化，行也是归一化的。则矩阵 $\\mathbf P^{(i,i)}$ 定义了在随机游走过程中，当前样本访问其他样本的概率。 Inter-relationships接下来我们定义不同的样本空间之间的inter-relationships，这里关系是基于co-occurrence information定义的。将第 $i$ 个样本空间中第 $k$ 个样本和第 $j$ 个样本空间的第 $l$ 个样本之间的关系定义为 $o_{k,l}^{(i,j)}$，则可以基于 $o_{k,l}^{(i,j)}$ 定义一个不同特征空间之间的关系矩阵 $\\mathbf A^{(i,j)}$，其大小为 $n_i-\\text{by}-n_j$。这里 $\\mathbf A^{(i,j)}$ 不一定是对称的，但是一定存在 $[\\mathbf A^{(i,j)}]_{(k,l)}=[\\mathbf A^{(j,i)}]$，这里 $\\mathbf A^{(j,i)}$ 是 $\\mathbf A^{(i,j)}$ 的转置。同样我们把 $\\mathbf A^{(i,j)}$ 针对列归一化，可以得到 $\\mathbf P^{(i,j)}$，注意这里某些列的和可能为0，因为两个样本空间之间存在某些我们找不到co-occurrence information的样本。这种情况下，就令这一列的每个元素值都等于 $\\frac 1 n_i$，从而得到了由一个样本空间到另一个样本空间的概率转移矩阵。 Coupled Markov-Chain由上述过程得到了所有样本空间中样本间的概率转移矩阵，因为有Intra-relationship和inter-relationships，把它们联合起来即为coupled，将两种概率转移矩阵联合起来可以构造成一个Markov Chain，即完成了我们最初的目的，就是为所有样本空间中的样本建立某种联系，从而为co-transfer做准备。 那具体怎么联合两种概率转移矩阵那，首先假设在时刻 $t$ 访问样本空间 $i$ 中的所有样本的概率构成一个概率分布向量(probability distribution vector)：$x^{(i)}(t)=[x_1^{(i)}(t),x_2^{(i)}(t),\\dots,x_{n_i}^{(i)}(t)]^{\\mathrm T}$，作为一个概率分布向量，它满足以下条件 $\\sum_{k=1}^{n_i}x_k^{(i)}(t)=1$。在coupled Markov chain中，我们考虑从 $\\lbrace x^{(i)}(t) \\rbrace_{i=1}^N$ 到 $\\lbrace x^{(i)}(t) \\rbrace_{i=1}^N$ 的一步转移概率，如式(6)所示：$$x^{(i)}(t+1)=\\sum_{j=1}^N\\lambda_{i,j}\\mathbf P^{(i,j)}x^{(j)}(t),\\quad i=1,2,\\dots,N.$$上式则表示在 $t+1$ 时刻，访问第 $i$ 个样本空间中样本的概率，等于在 $t$ 时刻从所有样本空间中的样本的转移概率加权和。这里为了保证得到的 $x^{(i)}(t+1)$ 仍然是一个概率分布向量，令 $\\sum_{j=1}^N\\lambda_{i,j}=1$。此处 $\\lambda_{i,j}$ 本质上是在控制从第 $i$ 个样本空间到第 $j$ 个样本空间迁移的知识量。 以矩阵的形式，可以把式(6)表示成式(7)的形式：$$x(t+1)=\\mathbf P x(t),\\quadx(t)=\\begin{pmatrix} x^{(1)}(t) \\\\ x^{(2)}(t) \\\\ \\vdots \\\\ x^{(N)}(t) \\end{pmatrix}, \\quad\\mathbf P=\\begin{pmatrix}\\lambda_{1,1}\\mathbf P^{(1,1)} &amp; \\lambda_{1,2}\\mathbf P^{(1,2)} &amp; \\dots &amp; \\lambda_{1,N}\\mathbf P^{(1,N)} \\\\\\lambda_{2,1}\\mathbf P^{(2,1)} &amp; \\lambda_{2,2}\\mathbf P^{(2,2)} &amp; \\dots &amp; \\lambda_{2,N}\\mathbf P^{(2,N)} \\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\\\lambda_{N,1}\\mathbf P^{(N,1)} &amp; \\lambda_{N,2}\\mathbf P^{(N,2)} &amp; \\dots &amp; \\lambda_{N,2}\\mathbf P^{(N,2)}\\end{pmatrix}$$此处求出的 $\\mathbf P$ 即为联合转移概率图(joint transition probability graph)。 Co-Transfer Learning给定上述的联合转移概率图后，我们假设一个random walker从已知标签的样本出发，这里的样本就表示图中的nodes。这个random walker根据 $\\mathbf P$ 重复迭代的访问它的neighborhood节点。在每个step中，它的return to training instances的概率是 $\\alpha$，walker最终达到一个steady-state稳定态 $\\mathbf U$，$\\mathbf U$ 最终能给出标签的ranking来表示每个测试样本的标签集的重要性。将上述问题形式化：$$(1-\\alpha)\\mathbf{PU}+\\alpha\\mathbf Q=\\mathbf U$$此处 $\\mathbf Q $ ($n-\\text {by}-c$) 是根据来自不用的样本空间中的所有训练数据构造的类标签的概率分布向量(probability distribution vector of the class labels)。这里的restart参数 $\\alpha$ 用于控制 $\\mathbf Q$ 对于最终的label ranking的重要性或者说影响。 因为 $\\mathbf P$ 是已知的，$\\mathbf U$ 是要求解的，所以现在重点就成了给定训练数据，怎么构造 $\\mathbf Q$，我们用以下式子来构造这个标签概率分布矩阵。$$\\begin{align}\\mathbf Q &amp;=[\\mathbf q_1,\\mathbf q_2,\\dots,\\mathbf q_c],\\\\\\mathbf q_d &amp;=[\\mathbf q_d^{(1)},q_d^{(2)},\\dots,q_d^{(N)}],\\\\\\mathbf q_d^{(i)} &amp;=[q_{d,1}^{(i)},q_{d,2}^{(i)},\\dots,q_{d,n_i}^{(i)}]\\end{align}$$ 上面式子中 $d=1,2,\\dots,c,$，这里 $c$ 表示类别数，则 $q_d^{(i)}$ 表示在第 $i$ 个样本空间中每个样本属于 $d$ 类的概率，即这里的 $\\mathbf q_d^{(i)}$ 是样本的概率分布向量，它是以如下方式由均匀分布得到的。$$q_{d,k}^{(i)}=\\lbrace \\begin{matrix}\\frac1{l_d^{(i)}},&amp;\\text{if }d\\in C_k^{(i)}, \\\\ 0, &amp; \\text{otherwise.}\\end{matrix}$$这里的 $q_{d,k}^{(i)}$ 表示样本空间 $i$ 中第 $k$ 个样本属于类别 $d$ 的概率，这里 $l_d^{(i)}$ 表示第 $i$ 个样本空间中属于类别 $d$ 的样本个数。 现在已知了 $\\mathbf Q$ 和 $\\mathbf P$，$\\mathbf U$ 可以通过迭代求解 $\\mathbf U(t)=(1-\\alpha)\\mathbf {PU}(t-1)$ 得到。则样本 $x_k^{(i)}$ 的标签可以通过矩阵 $\\mathbf U$ 对应这个样本那一行概率向量来预测，即如下：$$l_k^{(i)}=[u_{k,1}^{(i)},u_{k,2}^{(i)},\\dots,u_{k,c}^{(i)}]$$对于single class标签预测，取式(13)中最大的数值对应的标签就可以。对于multiclass标签预测，我们对这些概率进行一个ranking，取前 $d’$ 个即可。 Experiments实验部分，式(7)中的 $\\lambda$ 是需要人为赋值的，另外还需要co-occurrence information对inter-relationships进行计算。 ReferenceWu Q , Ng M K , Ye Y . Cotransfer Learning Using Coupled Markov Chains with Restart. IEEE Intelligent Systems, 2014, 29(4):26-33. Wu Q , Ng M K , Ye Y . Co-Transfer Learning via Joint Transition Probability Graph Based Method. in KDD, 2012.","categories":[{"name":"transfer learning","slug":"transfer-learning","permalink":"http://yoursite.com/categories/transfer-learning/"}],"tags":[{"name":"transfer learning","slug":"transfer-learning","permalink":"http://yoursite.com/tags/transfer-learning/"}]}]}