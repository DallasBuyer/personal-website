{"meta":{"title":"DallasBuyer","subtitle":"artificial intelligence","description":null,"author":"PengXu","url":"http://yoursite.com","root":"/"},"pages":[{"title":"Biography","date":"2019-04-24T04:54:31.000Z","updated":"2019-07-20T07:42:00.898Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"Peng Xu 许鹏Postgraduate StudentSoftware EngineeringSchool of Digital Media, Jiangnan UniversityEmail: dallasbuyer [AT] stu.jiangnan.edu.cnWeChat: xu172447385 InterestsI am interested in artificial intelligence, transfer learning, multi-view learning, fuzzy machine learning and interpretable machine learning. PublicationsConcise Fuzzy System Modeling Integrating Soft Subspace Clustering and Sparse LearningPeng Xu, Zhaohong Deng, Chen Cui, Te Zhang, Kup-Sze Choi, Suhang Gu, Shitong WangIEEE Transactions on Fuzzy System, 2019. [PDF] Transfer Representation Learning with TSK Fuzzy SystemPeng Xu, Zhaohong Deng, Jun Wang, Qun Zhang,Shitong WangarXiv:1901.02703, 2019. [PDF] Multi-view Information-theoretic Co-clustering for Co-occurrence DataPeng Xu, Zhaohong Deng, Kup-Sze Choi, Longbin Cao, Shitong WangAAAI (spotlight), 2019. [PDF] [CODE] Joint Information Preservation for Heterogeneous Domain AdaptationPeng Xu, Zhaohon Deng, Kup-Sze Choi, Jun Wang, Shitong WangTechical Report, 2019. [PDF] Deep Image Feature Learning with Fuzzy FulesXiang Ma, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangTechical Report, 2019. [PDF] Transductive Joint-Knowledge-Transfer TSK FS for Recognition of Epileptic EEG SignalsZhaohong Deng, Peng Xu, Lixiao Xie, Kup-Sze Choi, Shitong WangIEEE Transactions on Neural Systems and Rehabilitation Engineering, 2018. [PDF] Genralized Hidden-Mapping Transductive Transfer Learning for Recognition of Epileptic Electroencephalogram SignalsLixiao Xie, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangIEEE Transactions on Cybernetics, 2018. [PDF] TalksSpotlight Talk on the Thirty-Third AAAI Conference on Artificial Intelligence, Hawaii, USA, 01/2019 Reviewer and PC MemberJournalsIEEE Transactions on Fuzzy SystemIEEE Transactions on Emergin Topics in Computational IntelligenceIEEE Transactions on Systems, Man and Cybernetics: SystemsNeurocomputing ConferenceThe Third International Conference on Biological Information and Biomedical Engineering (BIBE 2019)The Twenty-Sixth International Conference on Neural Information Processing (ICONIP2019) Movie Recommendation"},{"title":"categories","date":"2019-04-25T10:41:10.000Z","updated":"2019-04-25T11:22:26.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"Publications and Code","date":"2019-04-25T10:41:10.000Z","updated":"2019-06-01T12:01:26.000Z","comments":true,"path":"publications/index.html","permalink":"http://yoursite.com/publications/index.html","excerpt":"","text":"Concise Fuzzy System Modeling Integrating Soft Subspace Clustering and Sparse LearningPeng Xu, Zhaohong Deng, Chen Cui, Te Zhang, Kup-Sze Choi, Suhang Gu, Shitong WangIEEE Transactions on Fuzzy System, 2019. [PDF] Transfer Representation Learning with TSK Fuzzy SystemPeng Xu, Zhaohong Deng, Jun Wang, Qun Zhang,Shitong WangarXiv:1901.02703, 2019. [PDF] Multi-view Information-theoretic Co-clustering for Co-occurrence DataPeng Xu, Zhaohong Deng, Kup-Sze Choi, Longbin Cao, Shitong WangAAAI (spotlight), 2019. [PDF] [CODE] Joint Information Preservation for Heterogeneous Domain AdaptationPeng Xu, Zhaohon Deng, Kup-Sze Choi, Jun Wang, Shitong WangTechical Report, 2019. [PDF] Deep Image Feature Learning with Fuzzy FulesXiang Ma, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangTechical Report, 2019. [PDF] Transductive Joint-Knowledge-Transfer TSK FS for Recognition of Epileptic EEG SignalsZhaohong Deng, Peng Xu, Lixiao Xie, Kup-Sze Choi, Shitong WangIEEE Transactions on Neural Systems and Rehabilitation Engineering, 2018. [PDF] Genralized Hidden-Mapping Transductive Transfer Learning for Recognition of Epileptic Electroencephalogram SignalsLixiao Xie, Zhaohong Deng, Peng Xu, Kup-Sze Choi, Shitong WangIEEE Transactions on Cybernetics, 2018. [PDF]"}],"posts":[{"title":"Exponential Smoothing","slug":"Exponential smoothing","date":"2019-07-20T08:01:43.000Z","updated":"2019-07-21T03:25:55.779Z","comments":true,"path":"2019/07/20/Exponential smoothing/","link":"","permalink":"http://yoursite.com/2019/07/20/Exponential smoothing/","excerpt":"指数平滑(exponential smoothing)和ARIMA一样也是一个使用很广泛的时间序列预测模型，基于指数平滑模型的预测对于过去观测量的一个加权平均，不过这里的加权平均和ARIMA不一样，并非线性加权，这里的权重会随着以往观测量变得久远而不断衰减。换句话说，就是距离当前预测越近的观测量，它的权重越高。","text":"指数平滑(exponential smoothing)和ARIMA一样也是一个使用很广泛的时间序列预测模型，基于指数平滑模型的预测对于过去观测量的一个加权平均，不过这里的加权平均和ARIMA不一样，并非线性加权，这里的权重会随着以往观测量变得久远而不断衰减。换句话说，就是距离当前预测越近的观测量，它的权重越高。 1. Simple exponential smoothing最简单的模型就是simple exponential smoothing(SME)，这个方法适用于那些没有clear trend or seasonal pattern的序列。如下图是几年间某个地区油的产量，这个序列并没有表现出很清晰的趋势或者周期性(当然最近几年有过一个上升阶段，这可能代表着某种趋势，这个以后再讨论有没有更高级的方法去建模)。我们现在只考虑用简单的naive和average方式做预测。 使用naive方法，有如下预测,这相当于忽略了之前所有信息，之用前一个，也就是把weight全都给了前一个时间点的值。 $$\\hat{y}_{T+h|T}=y_T$$ 使用average方法，则有如下预测，就相当于给之前预测都是平均的weight。 $$y_{T+h|T}=\\frac{1}{T} \\sum_{t=1}^T y_t$$ 上述两个其实是两个极端现象，更合理的情况是我们想让离预测点近的值权重更大，而离的远的值权重小一些。这其实也是SEM背后隐藏的思想，所以可以用如下方式对SEM建模。 $$\\hat{y}_{T+1|T}=\\alpha y_T+\\alpha(1-\\alpha)y_{T-1}+\\alpha(1-\\alpha)^2y_{T-2}+\\ldots$$ 参数$0&lt;\\alpha&lt;1$就是smoothing parameter，所以权重衰减是由参数$\\alpha$控制的。下面是参数$\\alpha$取不同值的权重衰减情况。可以看出只要参数在0到1之间，权重就会随时间的变化呈现指数形式不断衰减，所以这也是exponential smoothing的由来。 下面我们给出两种和上述SEM模型等价的表达式： 1.1. Weighted Average Form$$\\hat{y}_{T+1|t} = \\alpha y_T + (1-\\alpha)\\hat{y}_{T|T-1}$$这其实是一种迭代的方式给出的公式，只要从序列的开始进行建模，逐层带进去最后的计算结果和标准的SEM形式一致。 1.2. Component Form另外一种方式叫做成分表达式，对于SEM来说，我们只有单一的成分也就是level，$l_t$，之后的其他方法可能会涉及其他的成分，比如趋势成分trend component $b_t$和周期成分seasonal component $s_t$，这种成分表达式包含一个预测式和对于每种成分的一个平滑等式，SEM的component form如下。 $$\\begin{aligned}\\text{Forecast equation}: &amp; \\quad \\hat{y}_{t+h|t} =l_t \\\\\\text{Smoothing equation}: &amp; \\quad l_t =\\alpha y_t+(1-\\alpha)l_{t-1}\\end{aligned}$$ 由上式可以看出其实在$t+1$时刻的预测值其实就是在$t$时间的$level$。如果我们把上式中的$l_t$和$l_{t-1}$都替换成$y_{t+1|t}$和$y_{t|t-1}$，就可以回复SEM的weighted average form。 1.3. Optimization上述模型中有平滑参数以及序列模型初始值需要选，但是认为选效果可能不好，这时候就和线性回归一样，可以用如下优化的方式求解，不过模型不是线性模型，所以无法给出解析解，要用软件包来求解。$$\\text{SSE}=\\sum_{t=1}^T(y_t-\\hat{y}_{t|t-1})^2$$ 2. Trend Methods2.1. Holt’s linear trend method前面提到了SEM模型的component形式里面只包含了level，而包含趋势的表达式则可以写成如下形式。 $$\\begin{aligned}\\text{Forecast equation}: \\quad &amp; y_{t+h|t} =l_t+hb_t \\\\\\text{Smoothing equation}: \\quad &amp; l_t =\\alpha y_t+(1-\\alpha)(l_{t-1}+b_{t-1}) \\\\\\text{Trend equation}: \\quad &amp; b_t = \\beta (l_t-l_{t-1})+(1-\\beta) b_{t-1}\\end{aligned}$$ 这里$b_t$就是指序列在时间$t$时候的趋势，趋势也是一种斜率其实，所以用$l_t-l_{t-1}$来表示。$0&lt;\\beta^*&lt;1$是趋势的平滑参数。 2.2. Damped trend methodsHolt’s linear trend method在预测的时候会产生一个问题，就是在无限地预测未来的时候，会表现出constant trend(increasing or decreasing)。所以就有了这个方法，通过引入一个参数，它可以dampens(抑制)这种constant trend。包含有这种dampen trend的方法已经被证实特别成功，几乎毋庸置疑的它是时间序列预测算法中最流行的一个。除了参数$\\alpha$和$\\beta^*$，这个方法还引入了一个参数叫做damping parameter $0&lt;\\phi&lt;1$。 $$\\begin{aligned}y_{t+h|t} &amp; = l_t+(\\phi+\\phi^2+\\ldots+\\phi^h)b_t \\\\l_t &amp; = \\alpha y_t+(1-\\alpha)(l_{t-1}+\\phi b_{t-1}) \\\\b_t &amp; = \\beta(l_t-l_{t-1})+(1-\\beta)\\phi b_{t-1}\\end{aligned}$$ 如果$\\phi=1$，那这个方法就退化成了与Holt’s linear trend method一样。而对于$0&lt;\\phi&lt;1$，它可以抑制这种constant trend。事实上，对于任意的$0&lt;\\phi&lt;1$，当$h\\to\\infty$时，预测都是收敛到$l_T+\\frac{\\phi}{1-\\phi}b_T$，也就是short-run forecast are trended while long-run forecasts are constant。实际应用在，一般设置$0.8&lt;\\phi&lt;0.98$。。如下图是当$\\phi=0.9$时，某个时间序列的两种方法的趋势预测图。","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"ARIMA-4 AutoRegressive Integrated Moving Average","slug":"ARIMA-4 AutoRegressive Integrated Moving Average","date":"2019-07-20T06:45:08.000Z","updated":"2019-07-20T07:25:58.257Z","comments":true,"path":"2019/07/20/ARIMA-4 AutoRegressive Integrated Moving Average/","link":"","permalink":"http://yoursite.com/2019/07/20/ARIMA-4 AutoRegressive Integrated Moving Average/","excerpt":"如果我们将自回归差分模型(differencing with autoregression)和移动平均模型(moving average model)做一个组合，就可以得到非周期性的ARIMA模型，这个模型是non-seasonal的。ARIMA是AutoRegressive Integrated Moving Average的缩写，这里integration其实是differencing的逆过程。注意这里ARIMA模型中的序列是做过差分的序列，所以预测器中既包含了延迟的(lagged)时间$y_t$，也包含了延迟的误差$\\epsilon_t$。","text":"如果我们将自回归差分模型(differencing with autoregression)和移动平均模型(moving average model)做一个组合，就可以得到非周期性的ARIMA模型，这个模型是non-seasonal的。ARIMA是AutoRegressive Integrated Moving Average的缩写，这里integration其实是differencing的逆过程。注意这里ARIMA模型中的序列是做过差分的序列，所以预测器中既包含了延迟的(lagged)时间$y_t$，也包含了延迟的误差$\\epsilon_t$。 Non-seasonal ARIMA models如下是它的公式。$$y_t^{‘}=c+\\phi_1y_{t-1}^{‘}+\\ldots+\\phi_py_{t-p}^{‘}+\\theta_1\\epsilon_{t-1}+\\ldots+\\theta_q\\epsilon_{t-q}+\\epsilon_t$$上式中的$y’_t$就表示做过差分的序列，而且可能不只是一次差分。我们把这个模型叫做$\\text{ARIMA}(p,d,q)$，此处参数解释如下： $p=$order of the autoregressive part $d=$degree of first diffencing involved $q=$order of the moving average part 在自回归模型和移动平均模型中的stationary和invertibility条件对于ARIMA模型同样适用，下面是由ARIMA模型引申出的一些具体模型： White noise: ARIMA(0,0,0) Random walk: ARIMA(0,1,0) 即自回归和移动平均的系数都为0，只做一次差分$y_t = y_{t-1}+\\epsilon_t$ Random walk with drift: ARIMA(0,1,0) 即$y_t = c+y_{t-1}+\\epsilon_t$ Autoregression: ARIMA(p,0,0) Moving average: ARIMA(0,0,q) 如果我们把模型用backshift notation表示出来：$$\\begin{aligned}(1-\\phi_1B-\\ldots-\\phi_pB^p)(1-B)^dy_t&amp;=c+(1+\\theta_1B+\\ldots+\\theta_qB^q)\\epsilon_t \\\\AR(p)*d \\ \\text{differences} &amp;= MA(q)\\end{aligned}$$ Understanding ARIMA models上述公式中的常数$c$和差分次数$d$对于模型影响很大，我们可以做如下分析： $c=0 \\ \\text{and} \\ d=0$，长期预测值会趋向于0 $c=0 \\ \\text{and} \\ d=1$，长期预测值会趋向于非0常数 $c=0 \\ \\text{and} \\ d=2$，长期预测值会变成一条直线 $c\\neq0 \\ \\text{and} \\ d=0$，长期预测值会趋向于数据的平均值 $c\\neq0 \\ \\text{and} \\ d=0$，长期预测值会变成一条直线 $c\\neq0 \\ \\text{and} \\ d=0$，长期预测值会变成二次抛物线","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"ARIMA-3 Autoregressive Models and Moving Average Models","slug":"ARIMA-3 Autoregressive model and Moving average models","date":"2019-07-19T14:43:12.000Z","updated":"2019-07-20T09:23:11.240Z","comments":true,"path":"2019/07/19/ARIMA-3 Autoregressive model and Moving average models/","link":"","permalink":"http://yoursite.com/2019/07/19/ARIMA-3 Autoregressive model and Moving average models/","excerpt":"在机器学习常用的回归模型中，我们使用预测变量(predictors)的线性组合来预测感兴趣的变量，而在自回归模型(autoregression)模型中，我们某个变量过去的取值来预测这个变量。所以这里自回归表示用自己预测自己。而与自回归模型不同，移动平均模型(moving average model)是以类似回归的形式使用过去的预测误差进行预测。","text":"在机器学习常用的回归模型中，我们使用预测变量(predictors)的线性组合来预测感兴趣的变量，而在自回归模型(autoregression)模型中，我们某个变量过去的取值来预测这个变量。所以这里自回归表示用自己预测自己。而与自回归模型不同，移动平均模型(moving average model)是以类似回归的形式使用过去的预测误差进行预测。 1. Autoregressive models则自回归模型可以表示成如下形式：$$y_t=c+\\phi_1y_{t-1}+\\phi_2y_{t-2}+ \\ldots + \\phi_py_{t-p}+\\epsilon_t$$这里$\\epsilon_t$表示白噪声，我们把上述模型叫做一个$p$阶的自回归模型，可以表示为AR(p) model。自回归模型可以极其灵活的处理很多种不同的时间序列模式。如下图所示展示了AR(1)模型和AR(2)模型。通过改变参数$\\phi_1,\\ldots,\\phi_p$可以产生不同的时间序列模型，而误差项$\\epsilon_t$只会影响序列的尺度，不会影响patterns。 上面两个图中，第一个AR(1)模型和第二个AR(2)模型的函数分别为：$$\\begin{aligned}AR(1):\\quad y_t &amp; = 18-0.8y_{t-1}+\\epsilon_t \\\\AR(2):\\quad y_t &amp; = 8+1.3y_{t-1}-0.7y_{t-2}+\\epsilon_t\\end{aligned}$$对于AR(1)模型： 当$\\phi_1=0$的时候，$y_t$等价于白噪声(white noise) 当$\\phi_1=1 \\ \\land \\ c=0$的时候，$y_t$相当于随机游走(random walk) 当$\\phi_1=1 \\ \\land \\ c\\neq0$的时候，，$y_t$相当于带有漂移的随机游走(random walk with drift) 当$\\phi_1&lt;0$的时候，$y_t$会在均值附近震荡(oscillate) 一般来说，为了约束自回归模型为stationary数据，我们可以约束自回归模型的参数。 对于AR(1)模型：$-1&lt;\\phi_1&lt;1$ 对于AR(2)模型：$-1&lt;\\phi_2&lt;1,\\phi_1+\\phi_2&lt;1,\\phi_2-\\phi_1&lt;1$ 当$p\\geq3$的时候，约束就会变得很复杂了。 2. Moving average models移动平均模型的形式如下：$$y_t=c+\\epsilon_t+\\theta_1\\epsilon_{t-1}+\\theta_2\\epsilon_{t-2}+\\ldots+\\theta_q\\epsilon_{t-q}$$这里$\\epsilon_t$是白噪声，我们把这个模型叫做q阶的MA(q)模型，当然我们其实无法观测到变量$\\epsilon_t$的取值，所以它不算是通常意义上的回归模型。它知识认为$y_t$的值可以由过去的预测误差值的加权移动平均得到。如下图展示了两个模型MA(1)和MA(2)。 上图中的两个模型MA(1)和MA(2)分别为:$$\\begin{aligned}MA(1): \\quad y_t &amp;= 20+\\epsilon_t+0.8\\epsilon_{t-1} \\\\MA(2): \\quad y_t &amp;= \\epsilon_t - \\epsilon_{t-1} + 0.8\\epsilon_{t-2}\\end{aligned}$$其实任何一个stationary $\\text{AR}(p)$模型都可以表示成一个$\\text{MA}(\\infty)$模型，我们可以推到以下AR(1)模型。$$\\begin{aligned}MA(1): \\quad y_t &amp;= \\phi_1y_{t-1}+\\epsilon_t \\\\&amp;= \\phi_1(\\phi_1y_{t-2}+\\epsilon_{t-1})+\\epsilon_t \\\\&amp;= \\phi_1^2y_{t-2}+\\phi_1\\epsilon_{t-1}+\\epsilon_t \\\\&amp;= \\phi_1^3y_{t-3}+\\phi_1^2\\epsilon_{t-2}+\\phi_1\\epsilon_{t-1}+\\epsilon_t \\\\&amp;= etc.\\end{aligned}$$如果$-1&lt;\\phi_1&lt;1$，随着$k$变大，$\\phi_1^k$的值会越变越小，最终我们得到如下公式。$$y_t=\\epsilon_t+\\phi_1\\epsilon_{t-1}+\\phi_1^2\\epsilon_{t-2}+\\phi_1^3\\epsilon_{t-3}+\\ldots$$即为一个$\\text{MA}(\\infty)$模型。如果里面的参数$\\phi_1$和$\\phi_2$有满足上面提到的约束，则这个MA模型是可逆的，它可以转化为一个AR模型，也就是我们可以将任何可逆的MA(q)模型，转化成$\\text{AR}(\\infty$)模型，而且我们不仅仅是为了转化，这个过此中也有一些很不错的数学性质。比如我们将一个MA(1)过程，$y_t=\\epsilon_t+\\theta_1\\epsilon_{t-1}$转化成它的相应$\\text{AR}(\\infty)$模型,则有如下性质：$$\\epsilon_t=\\sum_{k=0}^\\infty(-\\theta)^jy_{t-j}$$此处未完待续…","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"ARIMA-2 Backshift Notation","slug":"ARIMA-2 Backshift Notation","date":"2019-07-19T06:27:38.000Z","updated":"2019-07-20T04:40:09.120Z","comments":true,"path":"2019/07/19/ARIMA-2 Backshift Notation/","link":"","permalink":"http://yoursite.com/2019/07/19/ARIMA-2 Backshift Notation/","excerpt":"在研究时间序列延迟的时候，后移符号(backshift notation)非常有用，可以用$B$表示后移操作。也有一些文献使用$L$表示lag而不是$B$表示backshift。换句话说，$B$在$y_t$上面的操作 就是将时序数据后移了一个周期，当然两个$B$就是后移了两个周期。","text":"在研究时间序列延迟的时候，后移符号(backshift notation)非常有用，可以用$B$表示后移操作。也有一些文献使用$L$表示lag而不是$B$表示backshift。换句话说，$B$在$y_t$上面的操作 就是将时序数据后移了一个周期，当然两个$B$就是后移了两个周期。 $$\\begin{aligned}By_t &amp; = y_{t-1} \\\\B(By_t) &amp; = B^2y_t=y_{t-2}\\end{aligned}$$ 对于monthly数据来说，如果我们想要表示去年同一个月的数据，就可以写成$B^{12}y_t=y_{t-12}$。后移操作对于描述differencing过程也非常的方便，如下。$$y^{‘}_t=y_t-y_{t-1}=y_t-By_t=(1-B)y_t$$所以由上述式子注意到一阶差分可以表示成$(1-B)$，同样的，二阶差分可以表示成如下形式。$$\\begin{aligned}y_t^{‘’} &amp; =y_t-2y_{t-1}+y_{t-2} \\\\&amp; =(1-2B+B)y_t \\\\&amp; =(1-B)^2y_t\\end{aligned}$$所以一般来说，一个$dth-order$的差分可以写成如下形式。$$(1-B)^dy_t$$后移操作在组合不同的差分计算是非常有用，它可以被当作一个适合于常用代数规则的运算符号。比如，当一个周期性差分紧跟着一个一阶差分时可以表示成如下形式。$$\\begin{aligned}(1-B)(1-B^{m})y_t &amp; =(1-B-B^m+B^{m+1})y_t \\\\ &amp; =y_t-y_{t-1}-y_{t-m}+y_{t-m-1}\\end{aligned}$$","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"ARIMA-1 Stationarity and Difference","slug":"ARIMA-1 Stationarity and Difference","date":"2019-07-19T01:59:05.000Z","updated":"2019-07-20T06:41:36.433Z","comments":true,"path":"2019/07/19/ARIMA-1 Stationarity and Difference/","link":"","permalink":"http://yoursite.com/2019/07/19/ARIMA-1 Stationarity and Difference/","excerpt":"ARIMA模型为时间序列预测提供了另外一种方法。在时间序列预测中exponential smoothing和ARIMA是两种最常用的模型，它们对于同一个问题具有互补的优势。Exponential smoothing模型可以描述数据中存在的trend和seasonality，而ARIMA模型可以描述数据中存在的autocorrelations。","text":"ARIMA模型为时间序列预测提供了另外一种方法。在时间序列预测中exponential smoothing和ARIMA是两种最常用的模型，它们对于同一个问题具有互补的优势。Exponential smoothing模型可以描述数据中存在的trend和seasonality，而ARIMA模型可以描述数据中存在的autocorrelations。 1. Stationarity一个stationary时间序列的性质和观察它的时间点无关，也就是它没有trends或者sensonality，因为trend和seasonality会在不同的时间点影响这时间序列的取值。比如white noise就是一个stationary序列，它的性质和你什么时候观察他没有关系。其实一个有这cyclic behaviour(but with no trend or seasonality)的时间序列也是一个stationary序列，这是因为cycles没有固定的长度，我们无法判断一个cycle的峰和谷。这里stationary序列指的是稳定序列，稳定序列的特性就是不受时间维度上各种特征的影响，之和自己之前的取值有关，这其实也是ARIMA算法的基础。比如股票数据一般来说其实就是不稳定数据，因为它常常受到政策，新闻或者整个经济大环境的影响。一般来说，stationary序列就是那些在long-term中没有predictable patterns的序列。下图是一些时间序列，请分析以下它们所属的类型，是不是stationary序列。 (a) 连续200天的谷歌股票价格 (b) 连续200天的谷歌股票价格的变化 (c) 美国每年恶性袭击的案件数 (d) 美国独栋别墅的月销量 (e) 美国一打鸡蛋的年平均价格 (f) 澳大利亚每个月杀猪量的总和 (g) 澳大利亚每年捕获猞猁的总量 (h) 澳大利亚每个月的啤酒产量 (i) 澳大利亚每个月的供电量 分析上图中的9个时间序列，很明显图(d),(h),(i)具有很强的季节性seasonality；图(a),(c),(e),(f),(i)具有很明显的趋势trends;图(i)的表现是方差越来越大；所以只剩下(b)和(g)是stationary series。乍一看好像图(g)表现出的cycles会使得它成为non-stationary序列，但是仔细观察发现这种cycles是aperiodic，所以在long-term内，这些cycles是无法预测的。 2. Differencing在上图中，(a)图是non-stationary，但是(b)图是stationary，这就提供了一种把序列从non-stationary转化到stationary的方式，就是计算连续观测值之间的差异，叫做differencing。Differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating(or reducing) trend and seasonality.使用ACF plot的方式可以有效地判断non-stationary时间序列。对于一个stationary时间序列，ACF will drop to zero relatively quickly while ACF of non-stationary data decreses slowly. 下图是谷歌股票价格和股票价格变化的ACF图。 则由上述讨论可知每天股票价格的变化是一个随机变量，并前几天没有什么相关性。 3. Random walk model上面提到的差异序列(differenced series)其实是原始序列的连续观测值之间的差值构成的，所以可以表达成如下形式。$$y_t^{‘}=y_t-y_{t-1}$$所以对一段长为$T$的序列来讲，它的差异序列的长度只有$T-1$个值。当差异序列是白噪声的时候，原始序列的模型可以用如下方式表达。$$y_t-y_{t-1}=\\epsilon _t$$这里$\\epsilon_t$表示白噪声，重新整理上式就得出了random walk随即游走模型。$$y_t=y_{t-1}+\\epsilon_t$$随机游走模型在non-stationary序列数据中有着广泛的应用。一般随机游走具有以下特点： long periods of apparent trends up or down sudden and unpredicted changes in direction 使用随机游走模型的预测值等于时间序列的最后一个观测值，因为继续随机游走的时候是unpredictable的，equally to be up or down。所以，随机游走模型underpins naive forecast method。一个类似的使得differences有非0均值的模型如下。$$y_t-t_{t-1}=c+\\epsilon_t \\quad \\text{or} \\quad y_t=c+y_{t-1}+\\epsilon_t$$这里的$c$是连续观测值之间变化的均值。如果$c$为正数，则平均变化值相对于$y_t$上升了，即$y_t$ will tend to drift upward，否则，$y_t$ will tend to drift downwards。则这时模型类似于drift method。 4. Second-order differencing有时differenced series还是没有表现出stationary的性质，这时候就很有必要再做一次difference以期待得到stationary序列。则有如下二阶差异：$$\\begin{aligned} y_t^{‘’} &amp; =y_t^{‘}-y_{t-1}^{‘} \\ &amp; = (y_t-y_{t-1})-(y_{t-1}-y_{t-2})\\ &amp; = y_t-2y_{t-1}+y_{t-2}\\end{aligned}$$这种情况下，$y_t^{‘’}$有$T-2$个值，在实际应用中，我们几乎不会使用大于second-order的differences。 5. Seasonal differencing周期性差异指的是当前观测和同周期的前一个观测之间的差异，可以如下表示。$$y_t^{‘}=y_t-y_{t-m}$$这里$m$表示周期的个数，上式也叫做lag-m differences。如果周期性差异数据是白噪声的化则有如下模型。$$y_t = y_{t-m}+\\epsilon_t$$则这个模型和naive forecast method是一样的。同第4小节中的二阶差异一样，周期性数据也存在二阶差异使其变成stationary数据。 6. Unit root test决定一个序列是否需要differencing的一种方式是unit root test单位根检验，这是一种决定是否应该使用differencing的假设检验方法。已有很多单位根检验方法，在这种假设检验中，空假设是数据是stationary的，如果假设被拒绝，则需要使用differencing方法，如果假设被接受则不需要。","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"Overview of Forecasting-Principles and Practice","slug":"overview: time series models","date":"2019-07-18T06:56:13.000Z","updated":"2019-07-20T04:39:19.488Z","comments":true,"path":"2019/07/18/overview: time series models/","link":"","permalink":"http://yoursite.com/2019/07/18/overview: time series models/","excerpt":"最近开始学习一些有关时间序列模型的内容，在此记录下来。此系列博客主要根据一本在线电子书的内容整理而来，为了目的是记录自己的学习历程，让自己的印象更加深刻。这是这本书的网页连接。这本数的题目叫做Forecasting:Principles and Practice，原来自己接触的大部分内容都属于机器学习，无非就是两大任务Prediction和Regreesion，而这里的Forecasting本人理解更倾向于是对时间序列进行预测，即预测未来某个时候挥发生的事情，而不是像分类和回归一样针对某个样本判断其类别或者是取值。","text":"最近开始学习一些有关时间序列模型的内容，在此记录下来。此系列博客主要根据一本在线电子书的内容整理而来，为了目的是记录自己的学习历程，让自己的印象更加深刻。这是这本书的网页连接。这本数的题目叫做Forecasting:Principles and Practice，原来自己接触的大部分内容都属于机器学习，无非就是两大任务Prediction和Regreesion，而这里的Forecasting本人理解更倾向于是对时间序列进行预测，即预测未来某个时候挥发生的事情，而不是像分类和回归一样针对某个样本判断其类别或者是取值。 1. What can be forecast?其实forecast有很多场景，比如预测未来5年整个国家需要的用电量，这会决定国家对于发电厂等基础设备的投资建设；比如预测某个公司下周客服中心的呼叫量，这会决定这个公司应该招收多少客服人员；再比如预测股市的走势以决定怎么投资，预测电商网站的销量以决定需要怎么调整供应链等等。这些场景都有一个共性的问题就是它们所产生的数据具有时序性。而一个事件的可预测性一般由以下几点决定： 我们是否充分理解导致预测结果的因素 我们能够获取多少以往的数据 预测是否会影响我们想要预测的事情 比如我们要预测用电量的话应该比较容易，因为首先我们有大量的数据，其次我们很理解是什么因素导致了用电量的变化，比如气温，节假日，经济环境等。但是想要预测汇率可能就有些困难了，因为我们虽然有大量的数据，但是我们不知道有哪些确定性的因素会影响汇率的变化。在forecasting中，很重要的一点是我们要知道什么时间我们能够精准的预测，什么时候预测的结果比投硬币要好。好的预测模型通常能够捕获历史数据中的patterns和relationships，而不是简单的复制过去发生的事情。我这里的关键就是我们采用什么样的模型能够把历史数据中的一些噪声给去掉，而提取出那些有效的patterns和relationships，并且根据未来发生事情的factors或者partial pattern来预测它的最终结果。 2. Forecasting, planning and goals其实forecasting在商业中是一个很常见的统计学任务，它可以为未来的产品规划，物流和人事，提供一个长期的战略计划。但是，在商业环境中forecasting一般完成的比较差劲而且还容易和另外两个内容混淆，就是planning和goals。 Forecasting: 是用已知的信息去尽可能准确去预测未来，这些已知信息包含historical data和knowledge of any future events that might impact the forecasts(factors)。 Goals: 目标是你想要它发生的事情，这里不涉及计划或者你的预测，只是你定的一个方向，能否实现，或者怎么实现都合目标无关。 Planning: 而计划就是为Forecasting和Goals负责的，计划就涉及根据预测来采取合适的行动从而让你的目标达成。 Forecasting是管理的决策行动中很关键的一部分，它为决策提供了根据，一般来说预测分为三类 Short-term forecasts: scheduling of personnel, production and transportation. Medium-term forecasts: future resource requirements, such as raw materials, hire personnel, or buy machinery and equipment. Long-term forecasts:strategic planning, including market opportunities, environmental factors and internal resources. 3. Determining what to forecast我们做预测的时候首先要清楚的就是预测什么，当面对很多历史数据时，要先根据商业环境和需求明确预测目的 every product line, or for groups of products? every sales outlet, or for outlets grouped by region, or only for total sales? weekly data, monthly data or annual data? 在确定的需要预测是什么之后紧接着的问题就是how frequent，我们需要每天更新预测数据吗，还是每周，每个月？在知道了这些之后才开始怎么预测的问题，就涉及到了methods和data。 4. Forecasting data and methods预测方法的选择取决于可获取的数据，如果我们无法获取到数据，那么可以采用qualitative forecasting(also known as judgmental forecasting)方法。而当以下条件满足时我们可以采用quantitative forecasting方法。 numerical information about the past is available; it is reasonabe to assume that some aspects of the past patterns will continue into the future. 有很多quantitative forecasting方法可以采用，一般来说大部分的quantitative prediction问题要么使用series data(collected at regular intervals over time)，要么使用cross-sectional data(collected at a single point in time)。而本书关注于time series domain。 4.1. Time series forecasting下面是一些例子： Daily IBM stock prices Monthly rainfall Quarterly sales results for Amazon Annual Google profits 本书只关注于一些固定区间时间长度的时间序列，比如hourly, daily, weekly, monthly, quarterly和annually。不关注irregularly spaced time series。最简单的时间序列预测方法就是仅仅根据以往的时序数据来预测未来的时序数据，而不去考虑导致最终结果的因素。常见的用于预测的时间序列模型包括decomposition models, exponential smoothing models and ARIMA models。 4.2. Predictor variables and time series forecasting一般来说，预测变量(predictor variables)在时间序列预测中很有效，假设我们现在想预测某个区域夏季一个小时的供电量，可以采用以下模型。$$\\mathrm{ED} = f(\\mathrm{current\\ temperature, strength\\ of\\ economy, population, time\\ of\\ day, day\\ of\\ week, error}).$$其实上述关系也不完全正确，因为供电量中总有一些变化是无法用上述预测变量来解释的，而最右侧的$\\mathrm{error}$项就可以代表随机波动和那些没有被包含到模型中的其他相关变量。我们可以叫上述为expanatory model，因为它可以帮助解释是什么引起了供电量需求的变化。因为随着时间变化供电量可以形成一个时间序列，所以我们也可以用一个如下的time series model来预测。$$\\mathrm{ED_{t+1}}=f(\\mathrm{ED_t, ED_{t-1},ED_{t-2}, ED_{t-3}, \\ldots,error})$$这里$t$表示当前一个小时，$t+1$表示下一个小时，$t-1$表示前一个小时。这里只采用了过去的时序数据，而没有外部变量。同样的，这里的$\\mathrm{error}$表示一些随机波动和没有包含到模型里面的相关变量。当然也有第三种模型结合前两种，可以叫做混合模型。$$\\mathrm{ED_{t+1}}=f(\\mathrm{ED_t, current\\ temperature, strength\\ of\\ economy, population, time\\ of\\ day, day\\ of\\ week, error})$$这类模型也叫做dynamic regression model, panel data models, longitudinal models, transfer functions models, linear system models。 5. The basic steps in a forecasting task Problem definition Gathering information Preliminary(exploratory analysis) Choosing and fitting models Using and evaluating a forecasting model","categories":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/categories/time-series-models/"}],"tags":[{"name":"time series models","slug":"time-series-models","permalink":"http://yoursite.com/tags/time-series-models/"}]},{"title":"Exploring Randomly Wired Neural Networks","slug":"random network generator","date":"2019-05-03T12:45:32.000Z","updated":"2019-07-20T04:39:38.360Z","comments":true,"path":"2019/05/03/random network generator/","link":"","permalink":"http://yoursite.com/2019/05/03/random network generator/","excerpt":"不同与以往的网络结构搜索算法，本文没有使用强化学习的方式令搜索出来的网络结构分类精度的reward很高。而是加不受限制的随机连接结构有怎样的表现。实验结果显示随机网络生成器所生成的网络结构的表现大多情况下能和人为设计的，通过NAS搜索得到的结果相媲美。一方面这篇文章提出了这种新的思想，就是设计不同的网络生成器；","text":"不同与以往的网络结构搜索算法，本文没有使用强化学习的方式令搜索出来的网络结构分类精度的reward很高。而是加不受限制的随机连接结构有怎样的表现。实验结果显示随机网络生成器所生成的网络结构的表现大多情况下能和人为设计的，通过NAS搜索得到的结果相媲美。一方面这篇文章提出了这种新的思想，就是设计不同的网络生成器；另一方面，结果表明用RL方式的暴力搜索其实并没有很大的优势，距离AutoML也还很遥远。由此启发我们从设计不同的搜索算法，搜索空间转而研究设计具有不同的先验知识的网络生成器。 1. Motivation现在大多数神经网络的成功在于其复杂的网络结构设计，为了探索网络结构对于深度学习的优势，网络结构搜索成了近一两年较为热门的方向。网络结构搜索 (neural architecture search, NAS)致力于同时优化网络的连接机构和操作算子，然而这类算法中网络结构的搜索过程都是认为设计的，所以网络结构的空间受到了很大的限制。所以，本文通过随机连接网络探索了更加多样性的网络结构。整个模型可以叫做随机网络生成器，实验结果显示，通过探索在更加不受约束的搜索空间中设计网络生成器会有不错的前景。 试验结果显示： 这些随机网络生成器的几个不同变种所产生的网络结构，都可以在Imagenet上面展现出竞争性的优势。其中最好的生成器能产生一些可以媲美甚至超过完全人为设计的或者是通过NAS搜索得到的网络结构。 同一个网络生成器所生成的网络精度方差较低。 不同的网络生成器生成的网络之间精度有明显差别。 上述讨论表明网络生成器的设计很关键。 2. Methodology为了扩大网络生成器的搜索空间，本文提出了随机连接神经网络。 先简要定义一下网络生成器，给定参数 $\\theta$ ，网络生成器 $g$ 会生成一个确定的网络，每次给定相同的 $\\theta$，$g$ 一定能产生相同的网络结构。如果网络生成器可以接受另外一个随机种子参数 $s$，通过这个随机种子，多次运行 $g(\\theta,s)$ 网络生成器可以产生一族网络结构，我们就叫这个生成器为随机网络生成器 $g(\\theta, s)$。(这里有个问题，每次给定相同的随机种子，随机网络生成器是不是也可以产生不同的网络结构)文献[1]中使用了循环神经网络RNN作为网络生成器，这里RNN可以输出一些列字符串，这些字符串定义的就是网络结构的超参数(比如，卷积网络每一层的kernel width，kernel height，stride width stride height，filter number)。这里训练这个神经网络是个难点，因为这里并不像传统RNN作为seq2seq模型有输入序列的监督信息可以使用反向传播来训练。本文作者采用了强化学习的策略梯度算法(policy gradient algorithm)中的REIFORCE算法，即使用reward来训练模型。本文中的reward指的是用这个RNN生成一个字符串网络后，训练并测试这个网络，这个网络的精度就是reward，而这个网络结构其实就是强化学习模型中的策略，也就是我们最后想学习的内容。(这里怎么用上述所提算法更新整个模型是个难点)文献[2]中也是使用循环神经网络作为网络生成器，它和[1]不同的是，它生成的是一个cell，这个cell由5个参数构成，即RNN可以生成5个参数，对应于5种不同的操作，这个RNN生成的每种操作的输出都是一个softmax分类层，类别是我们预先定义好的不同的操作类型。所以本文做的网络搜索其实是在搜索一种有效的cell结构，之后再将很多个cell拼接起来构成一个网络。这里采用的学习算法和[1]中一样，都是REINFORCE算法。上述两种网络搜索框架的缺陷是，它们都通过预定义的内容限制了网络的搜索空间，比如文献[1]中的搜索方式虽然可以产生多变的卷积核大小，步长，通道数，但是层与层之间的连接方式很受限制。文献[2]中的搜索结构是cell，预定义的内容直接限制了cell的构成结构，只不过一个cell中的操作算法可以变化。总之传统的NAS算法的搜索空间会被很多先验知识所限定。本文提出了随机连接网络，从而使得网络结构的搜索空间受到更少的限制，但是这里其实操作算子还是受到一定程度的限制。 3. Random Neural Generator本文使用随机图定义的随机网络生成器来生成不同的网络结构，本文选择了三种不同的已有随机图模型，在生成图模型之后，将图模型转化成网络结构。所以本文的两个核心问题点就是： 使用什么样的随机图模型生成随机图结构 怎么将图结构转化成对应的网络结构 3.1. Random Graph本文选择了三种随机图模型，分别是ER，BA，WS，下面简要讲解以下三种随机图模型。 ER是最简单的随机图模型，它假设给定N个节点，然后迭代地以概率P对任意两个节点进行连接，在遍历完所有的配对节点之后救生成了一个随机图。所以这个随机模型只有一个参数就是P。 BA算法以添加新节点的方式生成随机图模型，假设一开始有M个节点，它们之间没有任何边的连接。接下来每新添加一个节点就添加M条新边，怎么添加这M条边那，当将新添加的节点连接到点 $v$ 时，它连接到这个点的概率正比于这个点的degree。就这样重复这个过程知道这个点拥有M条边，之后以同样的方式添加另外的节点，直到一共有N个节点。所以最终生成的图一共有$M*(N-M)$ 条边。所以这个随机模型的唯一参数就是M。 WS是它们三个中较为复杂的一个随机模型。首先N个节点呈环形摆放，给定一个偶数K，每个节点都与其相邻的K/2个节点相连接。之后按顺时针顺序，对于每个节点 $v$，连接这个节点和它下面的第 $i$ 个节点的边都以概率 $P$ 重新连接。这里第 $i$ 个节点是以均匀分布的方式随机选择的，不过即不选择节点 $v$ 也不选择重复的边，这个选择过程重复K/2次。所以这个随机模型的参数有两个，分别是K和P。 3.2. Generate Network Structures from Graphs用上述三种方式生成图之后是无向图，首先我们需要将其转化成有向图模型。转换时现将每个图的所有节点附上序号，然后根据这些序号，使得每条边的方向都是从序号小的节点指向序号大的节点。那又按照什么策略给这些点赋序号那：对于ER模型，随机赋值。对于BA模型，初始的M个节点赋值为1到M，其余节点按其添加顺序赋值。WS模型，按照顺时针方向对所有节点赋值。得到有向图之后，就要将其转化成网络结构。这里假设边就是数据流，代表一个节点的数据流向两外一个节点。节点就是运算操作，每个节点都会受到多条边的输入，也会产生多条边的输出。这里我们假设一个节点就由三种操作构成，分别是聚合操作(aggregation)，转换操作(transformation)和分发操作(distribution)。聚合操作就是几条输入边的数据的加权和，这里存在可学习的权值。转换操作就是一个ReLU-Conv-BN的三联操作。分发操作就是将节点数据复制多份分别传输到不同的边上。由上述过程可知，其实虽然所提算法在网络结构上的搜索空间更大了，但是在操作上也严重限制了搜索空间，因为由生成的图转化成网络的过程中，所有运算都是固定的。 这里为了提高模型的能力，进行的分阶段构造，上述过程生成的一个图算是传统网络的一层，这样堆叠多个图，就构成了一个多层的网络。 4. Design and Optimization整个算法过程涉及的参数其实并不多。有每个图的节点数N，和每种随机图的参数。因为我们最终作了图的堆叠，所以还需要设置堆叠的图的个数，还有每个节点中的卷积操作的输出通道数和卷积步长。 5. Reference[1] Zoph B, Le Q V. Neural Architecture Search with Reinforcement Learning[J]. international conference on learning representations, 2017.[2] Zoph B, Vasudevan V, Shlens J, et al. Learning Transferable Architectures for Scalable Image Recognition[J]. computer vision and pattern recognition, 2018: 8697-8710.[3] Exploring Randomly Wired Neural Networks for Image Recognition. CoRR: abs/1904.01569","categories":[{"name":"neural architecture search","slug":"neural-architecture-search","permalink":"http://yoursite.com/categories/neural-architecture-search/"}],"tags":[{"name":"random neural network, neural architecture search","slug":"random-neural-network-neural-architecture-search","permalink":"http://yoursite.com/tags/random-neural-network-neural-architecture-search/"}]},{"title":"Co-Transfer Learning Using Coupled Markov Chains","slug":"Co-Transfer-Learning","date":"2019-04-25T08:21:47.000Z","updated":"2019-07-20T05:53:55.147Z","comments":true,"path":"2019/04/25/Co-Transfer-Learning/","link":"","permalink":"http://yoursite.com/2019/04/25/Co-Transfer-Learning/","excerpt":"个人感觉这篇文章很有新意，扩展了传统的迁移学习场景，和归纳式迁移学习一样，假设目标域也有标签数据，但是它的创新点在于co-transfer，即进行了双向迁移，使得任何一个领域都可以辅助其他领域迁移。而作者实现co-transfer在于利用了两种关系，一种是Intra-relationship，这里可以直接在同一个特征空间内用距离度量来构造；另外一种是inter-relationships，这里很巧妙地借助了co-occurrence information，","text":"个人感觉这篇文章很有新意，扩展了传统的迁移学习场景，和归纳式迁移学习一样，假设目标域也有标签数据，但是它的创新点在于co-transfer，即进行了双向迁移，使得任何一个领域都可以辅助其他领域迁移。而作者实现co-transfer在于利用了两种关系，一种是Intra-relationship，这里可以直接在同一个特征空间内用距离度量来构造；另外一种是inter-relationships，这里很巧妙地借助了co-occurrence information，注意这里的co-occurrence和多视角学习中的instance-level co-occurrence并不相同。 之后将两种关系合并构造一个coupled Markov chain形成了一个联合转移概率图，之后通过向标签分布的概率转移来实现标签预测，这里有趣的是还引入了多标签和标签ranking的概念，有兴趣的同学也可以借鉴这种思想解决多标签分类的问题。 Scene and Motivation本文提出了一个叫作Co-Transfer Learning的算法，这里的Co-Transfer意为协同迁移。传统的迁移学习场景一般假设利用包含大量有标签数据的源域辅助只包含少量有标签数据的目标域数据进行建模，之后对目标域的无标签数据进行分类。显然传统的迁移学习是单向的，即源域辅助目标域。而本文的Co-Transfer的出发点是能不能作多向的迁移，即没有源域和目标域之分。假设同时有多个领域的数据，其中每个领域都包含一部分有标签数据和一部分无标签数据，本文的目标就是这些领域互相迁移。举个例子，假设现在有文本和图像两个领域的数据，每个领域的数据都包含有标签和无标签两种数据，本文想通过某种方式为这两个领域的数据建立某种桥梁，从而让图像领域的数据辅助文本领域数据的建模，同时文本领域的数据也可以辅助图像领域的建模。如果存在多个领域的话就是多向的，每个领域都可以辅助其它任何一个领域进行建模，从而形成了Co-Transfer的思想。 Problem Definition有了以上出发点，现在就要把问题数学化。首先定义多个领域，这里每个领域定义为一个样本空间和一个特征空间，且每个领域的样本空间和特征空间都不同。将第 $i$ 个样本空间表示为 $\\mathcal{X^{(i)}}$，第 $i$ 个特征空间表示为 $\\mathcal{Y^{(i)}}$，则第 $i$ 个样本空间的某个样本表示为 $x_k^{(i)}$，其对应的特征向量表示为 $\\mathbf{y}_k^{(i)}$ 。且对于任意的 $i\\neq i’$，有 $\\mathcal{X^{(i)}}\\neq \\mathcal{X^{(i’)}}$ 且 $\\mathcal{Y^{(i)}}\\neq \\mathcal{Y^{(i’)}} $，即每两个领域的样本空间和特征空间均不相同。每个领域都存在一个有标签样本集 $\\mathcal{L^{(i)}}$ 和一个无标签样本集 $\\mathcal{U^{(i)}}$。其中 $\\mathcal{L^{(i)}}$ 可用式(1)表示，${\\bar n_i}$ 第 $i$ 个领域有标签样本的个数。$\\mathcal{U^{(i)}}$ 可用式(2)表示，$\\hat n_i$ 表示第 $i$ 个领域无标签样本的个数。Co-Transfer Learning的目的就是使用所有领域中的有标签样本同时训练 $N$ 个不同的分类器来预测无标签测试样本的类别。$$\\mathcal {L^{(i)}}={ x_k^{(i)},C_k^{(i)} }_{k=1}^{\\bar n_i}$$ $$\\mathcal {U^{(i)}}={ x_k^{(i)} }_{k=\\bar n_i+1}^{\\bar n_i+\\hat n_i}$$ The Proposed Framework本文所提方法将问题建模成一个所有样本的联合转移概率图(joint transition probability graph)模型，这样就可以利用topic-sensitive PageRank和random walk with restart的思想来解决co-transfer的问题了。其中transition probabilities可以通过intra-relationships和inter-relationships来构造，这里intra-relationships是指在同一个样本空间内所有样本的关系，它可以基于样本之间的affinity metric来构造，inter-relationships是指在不同样本空间的样本之间的关系，它可以基于样本之间的co-occurrence information来构造。未完待续… Intrarelationships and Interrelationships式(3)展示出了所有 $N$ 个样本空间的所有样本(有标签和无标签)，第 $i$ 个样本空间的所有样本数为 $n_i=\\bar n_i+\\hat n_i$。为了简化表示，我们设 $n=\\sum _{i=1}^Kn_i$。$$\\underbrace{x_1^{(1)},\\dots,x_{n_1}^{(1)}}_{\\text{1st instance space}},\\underbrace{x_1^{(2)},\\dots,x_{n_2}^{(2)}}_{\\text{2st instance space}},\\dots,\\underbrace{x_1^{(N)},\\dots,x_{n_N}^{(N)}}_{\\text{Nst instance space}}$$ Intra-relationship首先我们定义在同一个样本空间内的所有样本之间的intra-relationships，在第 $i$ 个样本空间内的样本 $x_k^{(i)}$ 和 $x_l^{(i)}$ 之间的affinity metric定义为 $a_{k,l}^{(i,i)}$ ， $a_{k,l}^{(i,i)}$ 可以基于两个样本在特征空间的特征表示来计算，如式(4)。$$a_{k,l}^{(i,i)}=exp[\\frac {-\\Vert \\mathbf y_k^{(i)}-\\mathbf y_l^{(i)}\\Vert_2}{2\\sigma^2}]$$从而所有的 $a_{k,l}^{(i,i)}$ 可以组合形成矩阵 $\\mathbf A^{(i,i)}$，其大小为 $n_i-\\text{by}-n_i$，其中的每个元素都代表此样本空间内两个样本之间的intra-relationship。这里重点来了，我们根据矩阵 $\\mathbf A^{(i,i)}$ 就可以构造一个Markov transition probability matrix $\\mathbf P^{(i,i)}$，怎么构造那，可以对 $\\mathbf A^{(i,i)}$ 的每一列进行归一化从而得到 $\\mathbf P^{(i,i)}$，使得 $\\mathbf P^{(i,i)}$ 的每一列元素的和都为1。因为这里矩阵 $\\mathbf A^{(i,i)}$ 是对称的，所以得到的 $\\mathbf P^{(i,i)}$ 不仅列实现了归一化，行也是归一化的。则矩阵 $\\mathbf P^{(i,i)}$ 定义了在随机游走过程中，当前样本访问其他样本的概率。 Inter-relationships接下来我们定义不同的样本空间之间的inter-relationships，这里关系是基于co-occurrence information定义的。将第 $i$ 个样本空间中第 $k$ 个样本和第 $j$ 个样本空间的第 $l$ 个样本之间的关系定义为 $o_{k,l}^{(i,j)}$，则可以基于 $o_{k,l}^{(i,j)}$ 定义一个不同特征空间之间的关系矩阵 $\\mathbf A^{(i,j)}$，其大小为 $n_i-\\text{by}-n_j$。这里 $\\mathbf A^{(i,j)}$ 不一定是对称的，但是一定存在 $[\\mathbf A^{(i,j)}]_{(k,l)}=[\\mathbf A^{(j,i)}]$，这里 $\\mathbf A^{(j,i)}$ 是 $\\mathbf A^{(i,j)}$ 的转置。同样我们把 $\\mathbf A^{(i,j)}$ 针对列归一化，可以得到 $\\mathbf P^{(i,j)}$，注意这里某些列的和可能为0，因为两个样本空间之间存在某些我们找不到co-occurrence information的样本。这种情况下，就令这一列的每个元素值都等于 $\\frac 1 n_i$，从而得到了由一个样本空间到另一个样本空间的概率转移矩阵。 Coupled Markov-Chain由上述过程得到了所有样本空间中样本间的概率转移矩阵，因为有Intra-relationship和inter-relationships，把它们联合起来即为coupled，将两种概率转移矩阵联合起来可以构造成一个Markov Chain，即完成了我们最初的目的，就是为所有样本空间中的样本建立某种联系，从而为co-transfer做准备。 那具体怎么联合两种概率转移矩阵那，首先假设在时刻 $t$ 访问样本空间 $i$ 中的所有样本的概率构成一个概率分布向量(probability distribution vector)：$x^{(i)}(t)=[x_1^{(i)}(t),x_2^{(i)}(t),\\dots,x_{n_i}^{(i)}(t)]^{\\mathrm T}$，作为一个概率分布向量，它满足以下条件 $\\sum_{k=1}^{n_i}x_k^{(i)}(t)=1$。在coupled Markov chain中，我们考虑从 $\\lbrace x^{(i)}(t) \\rbrace_{i=1}^N$ 到 $\\lbrace x^{(i)}(t) \\rbrace_{i=1}^N$ 的一步转移概率，如式(6)所示：$$x^{(i)}(t+1)=\\sum_{j=1}^N\\lambda_{i,j}\\mathbf P^{(i,j)}x^{(j)}(t),\\quad i=1,2,\\dots,N.$$上式则表示在 $t+1$ 时刻，访问第 $i$ 个样本空间中样本的概率，等于在 $t$ 时刻从所有样本空间中的样本的转移概率加权和。这里为了保证得到的 $x^{(i)}(t+1)$ 仍然是一个概率分布向量，令 $\\sum_{j=1}^N\\lambda_{i,j}=1$。此处 $\\lambda_{i,j}$ 本质上是在控制从第 $i$ 个样本空间到第 $j$ 个样本空间迁移的知识量。 以矩阵的形式，可以把式(6)表示成式(7)的形式：$$x(t+1)=\\mathbf P x(t),\\quadx(t)=\\begin{pmatrix} x^{(1)}(t) \\\\ x^{(2)}(t) \\\\ \\vdots \\\\ x^{(N)}(t) \\end{pmatrix}, \\quad\\mathbf P=\\begin{pmatrix}\\lambda_{1,1}\\mathbf P^{(1,1)} &amp; \\lambda_{1,2}\\mathbf P^{(1,2)} &amp; \\dots &amp; \\lambda_{1,N}\\mathbf P^{(1,N)} \\\\\\lambda_{2,1}\\mathbf P^{(2,1)} &amp; \\lambda_{2,2}\\mathbf P^{(2,2)} &amp; \\dots &amp; \\lambda_{2,N}\\mathbf P^{(2,N)} \\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\\\lambda_{N,1}\\mathbf P^{(N,1)} &amp; \\lambda_{N,2}\\mathbf P^{(N,2)} &amp; \\dots &amp; \\lambda_{N,2}\\mathbf P^{(N,2)}\\end{pmatrix}$$此处求出的 $\\mathbf P$ 即为联合转移概率图(joint transition probability graph)。 Co-Transfer Learning给定上述的联合转移概率图后，我们假设一个random walker从已知标签的样本出发，这里的样本就表示图中的nodes。这个random walker根据 $\\mathbf P$ 重复迭代的访问它的neighborhood节点。在每个step中，它的return to training instances的概率是 $\\alpha$，walker最终达到一个steady-state稳定态 $\\mathbf U$，$\\mathbf U$ 最终能给出标签的ranking来表示每个测试样本的标签集的重要性。将上述问题形式化：$$(1-\\alpha)\\mathbf{PU}+\\alpha\\mathbf Q=\\mathbf U$$此处 $\\mathbf Q $ ($n-\\text {by}-c$) 是根据来自不用的样本空间中的所有训练数据构造的类标签的概率分布向量(probability distribution vector of the class labels)。这里的restart参数 $\\alpha$ 用于控制 $\\mathbf Q$ 对于最终的label ranking的重要性或者说影响。 因为 $\\mathbf P$ 是已知的，$\\mathbf U$ 是要求解的，所以现在重点就成了给定训练数据，怎么构造 $\\mathbf Q$，我们用以下式子来构造这个标签概率分布矩阵。$$\\begin{align}\\mathbf Q &amp;=[\\mathbf q_1,\\mathbf q_2,\\dots,\\mathbf q_c],\\\\\\mathbf q_d &amp;=[\\mathbf q_d^{(1)},q_d^{(2)},\\dots,q_d^{(N)}],\\\\\\mathbf q_d^{(i)} &amp;=[q_{d,1}^{(i)},q_{d,2}^{(i)},\\dots,q_{d,n_i}^{(i)}]\\end{align}$$ 上面式子中 $d=1,2,\\dots,c,$，这里 $c$ 表示类别数，则 $q_d^{(i)}$ 表示在第 $i$ 个样本空间中每个样本属于 $d$ 类的概率，即这里的 $\\mathbf q_d^{(i)}$ 是样本的概率分布向量，它是以如下方式由均匀分布得到的。$$q_{d,k}^{(i)}=\\lbrace \\begin{matrix}\\frac1{l_d^{(i)}},&amp;\\text{if }d\\in C_k^{(i)}, \\\\ 0, &amp; \\text{otherwise.}\\end{matrix}$$这里的 $q_{d,k}^{(i)}$ 表示样本空间 $i$ 中第 $k$ 个样本属于类别 $d$ 的概率，这里 $l_d^{(i)}$ 表示第 $i$ 个样本空间中属于类别 $d$ 的样本个数。 现在已知了 $\\mathbf Q$ 和 $\\mathbf P$，$\\mathbf U$ 可以通过迭代求解 $\\mathbf U(t)=(1-\\alpha)\\mathbf {PU}(t-1)$ 得到。则样本 $x_k^{(i)}$ 的标签可以通过矩阵 $\\mathbf U$ 对应这个样本那一行概率向量来预测，即如下：$$l_k^{(i)}=[u_{k,1}^{(i)},u_{k,2}^{(i)},\\dots,u_{k,c}^{(i)}]$$对于single class标签预测，取式(13)中最大的数值对应的标签就可以。对于multiclass标签预测，我们对这些概率进行一个ranking，取前 $d’$ 个即可。 Experiments实验部分，式(7)中的 $\\lambda$ 是需要人为赋值的，另外还需要co-occurrence information对inter-relationships进行计算。 ReferenceWu Q , Ng M K , Ye Y . Cotransfer Learning Using Coupled Markov Chains with Restart. IEEE Intelligent Systems, 2014, 29(4):26-33. Wu Q , Ng M K , Ye Y . Co-Transfer Learning via Joint Transition Probability Graph Based Method. in KDD, 2012.","categories":[{"name":"transfer learning","slug":"transfer-learning","permalink":"http://yoursite.com/categories/transfer-learning/"}],"tags":[{"name":"transfer learning","slug":"transfer-learning","permalink":"http://yoursite.com/tags/transfer-learning/"}]}]}